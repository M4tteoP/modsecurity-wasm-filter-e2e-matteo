name: test
on:
  push:
    branches:
      - master
    paths-ignore:
      - "**/*.md"
      - "LICENSE"
  pull_request:

jobs:
  build-wasm-module:
    runs-on: ubuntu-latest
    steps:
      - name: "Checkout"
        uses: actions/checkout@v2
        with:
          submodules: recursive

      - name: "Retriving modsecurity-wasm-filter hash"
        # Cache's key based on the modesecurity-wasm-filter repo, it should lead to
        # rebuild the wasm module only if there are updates on that repo.
        run: echo "CACHE_MODSEC_WASM_FILTER_HASH=${{ hashFiles('modsecurity-wasm-filter/**') }}" >> $GITHUB_ENV

      - name: "Attempts to restore cache"
        id: cache-wasm-bin
        uses: actions/cache@v3
        with:
          path: |
            build/
          key: wasm-module-build-${{ env.CACHE_MODSEC_WASM_FILTER_HASH }}

      - name: "Build wasm module"
        if: steps.cache-wasm-bin.outputs.cache-hit != 'true'
        shell: bash
        run: make build-wasm-plugin extract-wasm-plugin

      - name: "Cache generated .wasm file"
        if: steps.cache-wasm-bin.outputs.cache-hit != 'true'
        uses: actions/cache@v3
        with:
          path: |
            build/
          key: wasm-module-build-${{ env.CACHE_MODSEC_WASM_FILTER_HASH }}

  e2e-local-tests:
    runs-on: ubuntu-latest
    needs: build-wasm-module
    steps:
      - name: "Checkout"
        uses: actions/checkout@v2
        with:
          submodules: recursive

      - name: "Install func-e"
        shell: bash
        run: curl https://func-e.io/install.sh | bash -s -- -b /usr/local/bin

      - name: "Restore the wasm files cache"
        uses: actions/cache@v3
        with:
          path: |
            build/
          key: wasm-module-build-${{ hashFiles('modsecurity-wasm-filter/**') }}

      - name: "Spin up local server and envoy"
        shell: bash
        run: |
          docker run --rm -d -p 8000:10080 jcchavezs/httpmole -response-status=200 &
          func-e run -c envoy-config.yaml --log-level info --component-log-level wasm:debug &

      - name: "Run local tests"
        env:
          APPLICATION_URL: "http://localhost:8000"
          REQ_UNFILTERED: "http://localhost:8001/home"
          REQ_FILTERED: "http://localhost:8001/admin"
        shell: bash
        run: |
          ./tests.sh


  e2e-istio-tests:
    runs-on: ubuntu-latest
    # write permission needed to upload the wasm to ghcr
    # Write permission must be also added via Package settings -> Manage Actions access
    permissions:
      packages: write 
    needs: build-wasm-module
    steps:
      - name: "Checkout"
        uses: actions/checkout@v2
        with:
          submodules: recursive
      
      - name: "Restore the wasm files cache"
        uses: actions/cache@v3
        with:
          path: |
            bin/
          key: wasm-module-build-${{ hashFiles('modsecurity-wasm-filter/**') }}
      
      - name: Install Kind
        shell: bash
        run: |
          curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.12.0/kind-linux-amd64
          sudo mv ./kind /usr/bin/kind 
      
      - name: Install istioctl
        shell: bash
        run: |
          curl -sL https://istio.io/downloadIstioctl | sh -

      - name: Create Kind cluster
      # kubectl is already installed on the Github Ubuntu worker
        run: |
          kind create cluster --config=./kind-istio/kind-config.yaml
          kubectl scale deployment --replicas 1 coredns --namespace kube-system

      - name: Install Istio
        run: |   
          yes | $HOME/.istioctl/bin/istioctl install --set profile=default
          kubectl label namespace default istio-injection=enabled --overwrite

      - name: Install metallb
        run: |   
          kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml
          kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml
          kubectl apply -f ./kind-istio/metallb-configmap.yaml

      - name: Deploy nginx and Gateway
        run: |   
          kubectl apply -f ./kind-istio/workload/deploy-nginx.yaml
          kubectl apply -f ./kind-istio/workload/istio-gateway.yaml

      - name: Build and Upload WASM-WAF to GHCR
        env:
          GITHUB_REPOSITORY_OWNER: ${{ github.repository_owner }}
      # Built it's compact variant and push it to ghcr.io
      # See https://github.com/tetratelabs/proxy-wasm-go-sdk/blob/main/Makefile#L62
      # See https://github.com/solo-io/wasm/blob/master/spec/spec-compat.md
        run: |
          export GITHUB_REPOSITORY_OWNER_LC=$(echo "$GITHUB_REPOSITORY_OWNER" | tr '[:upper:]' '[:lower:]')
          echo "GLOBAL_OWNER_LC=$GITHUB_REPOSITORY_OWNER_LC" >> $GITHUB_ENV
          echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u $GITHUB_REPOSITORY_OWNER --password-stdin
          docker build -t "ghcr.io/$GITHUB_REPOSITORY_OWNER_LC/modsecurity-filter" . -f wasm-image.Dockerfile --build-arg WASM_BINARY_PATH=./bin/modsecurity-filter.wasm
          docker push ghcr.io/$GITHUB_REPOSITORY_OWNER_LC/modsecurity-filter:latest

      - name: Deploy WASM-WAF
      # keep the GHCR owner as a variable. The name must be lowercase.
      # Save it globally GLOBAL_OWNER_LC
        run: |
          export GITHUB_REPOSITORY_OWNER_LC=$(echo "${{ env.GLOBAL_OWNER_LC }}")
          sed -i "s/{{GITHUB_REPOSITORY_OWNER}}/$GITHUB_REPOSITORY_OWNER_LC/" kind-istio/workload/deploy-waf.yaml  
          kubectl apply -f kind-istio/workload/deploy-waf.yaml

      - name: Run sanity checks
        run: |
          sleep 20
          echo "post WAF sleep"
          kubectl get pods -o wide
          kubectl logs deployment/nginx-v1 -c istio-proxy
          curl -v "http://172.18.0.100/nginx"
          kubectl get svc -o wide -n istio-system
          curl -v "http://172.18.0.100/admin"

      - name: "Run waf on istio tests"
        env:
          APPLICATION_URL: "http://172.18.0.100/nginx"
          REQ_UNFILTERED: "http://172.18.0.100/nginx"
          REQ_FILTERED: "http://172.18.0.100/admin"
        shell: bash
        # TODO: after removing the sanity checks, here must be added a script that waits until everything is read
        run: |
          ./tests.sh

